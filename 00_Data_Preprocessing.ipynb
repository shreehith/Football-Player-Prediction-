{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' Football Player Performance - Data Preprocessing\\n\\nThis notebook handles data cleaning, feature engineering, and preparation for the 5 main objectives:\\n\\n1. **Player Rating Prediction**\\n2. **Goals/Assists Prediction** \\n3. **Player Potential Evaluation**\\n4. **Training Plan Recommendations**\\n5. **Tactics/Formation Recommendations**\\n\\n## Data Sources:\\n- Player statistics from 8 major football leagues\\n- 12 different CSV files per team covering various performance aspects\\n- Historical match data and individual player metrics\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "''' Football Player Performance - Data Preprocessing\n",
        "\n",
        "This notebook handles data cleaning, feature engineering, and preparation for the 5 main objectives:\n",
        "\n",
        "1. **Player Rating Prediction**\n",
        "2. **Goals/Assists Prediction** \n",
        "3. **Player Potential Evaluation**\n",
        "4. **Training Plan Recommendations**\n",
        "5. **Tactics/Formation Recommendations**\n",
        "\n",
        "## Data Sources:\n",
        "- Player statistics from 8 major football leagues\n",
        "- 12 different CSV files per team covering various performance aspects\n",
        "- Historical match data and individual player metrics\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Preprocessing Module Initialized\n",
            "Working with 8 leagues\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import warnings\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "base_path = \"archive (3)\"\n",
        "leagues = [\"EPL\", \"La Liga\", \"Bundesliga\", \"Serie A\", \"Ligue 1\", \"Eredivise\", \"Segunda Division\", \"Brasil Serie A\"]\n",
        "\n",
        "print(\"Data Preprocessing Module Initialized\")\n",
        "print(f\"Working with {len(leagues)} leagues\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Data Loading Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== LOADING SAMPLE DATA ===\n",
            "Loading players.csv...\n",
            "  âœ“ Loaded 5942 records from 157 teams\n",
            "Sample data loaded: ['players']\n",
            "players: (5942, 38)\n",
            "Columns: ['Player', 'Nation', 'Pos', 'Age', 'Playing Time MP', 'Playing Time Starts', 'Playing Time Min', 'Playing Time 90s', 'Performance Gls', 'Performance Ast', 'Performance G+A', 'Performance G-PK', 'Performance PK', 'Performance PKatt', 'Performance CrdY', 'Performance CrdR', 'Expected xG', 'Expected npxG', 'Expected xAG', 'Expected npxG+xAG', 'Progression PrgC', 'Progression PrgP', 'Progression PrgR', 'Per 90 Minutes Gls', 'Per 90 Minutes Ast', 'Per 90 Minutes G+A', 'Per 90 Minutes G-PK', 'Per 90 Minutes G+A-PK', 'Per 90 Minutes xG', 'Per 90 Minutes xAG', 'Per 90 Minutes xG+xAG', 'Per 90 Minutes npxG', 'Per 90 Minutes npxG+xAG', 'Matches', 'League', 'Team', 'Data_Source', 'MP']\n"
          ]
        }
      ],
      "source": [
        "def load_all_data(file_type=\"all\"):\n",
        "    \"\"\"\n",
        "    Load data from all teams across all leagues\n",
        "    \n",
        "    Parameters:\n",
        "    file_type: str - specific file type to load, or \"all\" for all files\n",
        "    \n",
        "    Returns:\n",
        "    dict: Dictionary with dataframes organized by file type\n",
        "    \"\"\"\n",
        "    \n",
        "    data_collection = {}\n",
        "    \n",
        "    file_types = [\n",
        "        \"players.csv\", \"matches.csv\", \"playing_time.csv\", \"shooting.csv\",\n",
        "        \"passing.csv\", \"defensive_actions.csv\", \"possession.csv\",\n",
        "        \"goalkeepers.csv\", \"advanced_goalkeeping.csv\", \"g_e_s_creation.csv\",\n",
        "        \"pass_types.csv\", \"miscellaneous_stats.csv\"\n",
        "    ]\n",
        "    \n",
        "    if file_type != \"all\":\n",
        "        file_types = [file_type] if file_type in file_types else []\n",
        "    \n",
        "    for file_name in file_types:\n",
        "        print(f\"Loading {file_name}...\")\n",
        "        all_data = []\n",
        "        \n",
        "        for league in leagues:\n",
        "            league_path = os.path.join(base_path, league)\n",
        "            if os.path.exists(league_path):\n",
        "                team_folders = [f for f in os.listdir(league_path) \n",
        "                              if os.path.isdir(os.path.join(league_path, f))]\n",
        "                \n",
        "                for team in team_folders:\n",
        "                    file_path = os.path.join(league_path, team, file_name)\n",
        "                    if os.path.exists(file_path):\n",
        "                        try:\n",
        "                            df = pd.read_csv(file_path)\n",
        "                            df['League'] = league\n",
        "                            df['Team'] = team\n",
        "                            df['Data_Source'] = f\"{league}_{team}\"\n",
        "                            all_data.append(df)\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error loading {file_path}: {e}\")\n",
        "        \n",
        "        if all_data:\n",
        "            combined_df = pd.concat(all_data, ignore_index=True)\n",
        "            data_collection[file_name.replace('.csv', '')] = combined_df\n",
        "            print(f\"  âœ“ Loaded {len(combined_df)} records from {len(all_data)} teams\")\n",
        "        else:\n",
        "            print(f\"  âœ— No data found for {file_name}\")\n",
        "    \n",
        "    return data_collection\n",
        "\n",
        "# Load sample data to understand structure\n",
        "print(\"=== LOADING SAMPLE DATA ===\")\n",
        "sample_data = load_all_data(\"players.csv\")\n",
        "if sample_data:\n",
        "    print(f\"Sample data loaded: {list(sample_data.keys())}\")\n",
        "    for key, df in sample_data.items():\n",
        "        print(f\"{key}: {df.shape}\")\n",
        "        print(f\"Columns: {list(df.columns)}\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== TESTING PREPROCESSING PIPELINE ===\n",
            "\n",
            "Using players dataset for testing...\n",
            "Original shape: (5942, 38)\n",
            "Columns: ['Player', 'Nation', 'Pos', 'Age', 'Playing Time MP', 'Playing Time Starts', 'Playing Time Min', 'Playing Time 90s', 'Performance Gls', 'Performance Ast']...\n",
            "\n",
            "ðŸ§¹ Testing basic data cleaning...\n",
            "Missing values found in 2 columns\n",
            "Data types: {dtype('float64'): 29, dtype('O'): 8, dtype('int64'): 1}\n",
            "\n",
            "Found 30 numeric and 8 categorical columns\n",
            "âœ… Basic cleaning completed!\n",
            "Shape after cleaning: (5942, 38)\n",
            "Missing values remaining: 0\n"
          ]
        }
      ],
      "source": [
        "# Example usage with error handling\n",
        "print(\"=== TESTING PREPROCESSING PIPELINE ===\")\n",
        "\n",
        "# Check if we have data loaded\n",
        "if 'sample_data' in globals() and sample_data:\n",
        "    # Get the first dataset\n",
        "    data_key = list(sample_data.keys())[0]\n",
        "    raw_df = sample_data[data_key]\n",
        "    \n",
        "    print(f\"\\nUsing {data_key} dataset for testing...\")\n",
        "    print(f\"Original shape: {raw_df.shape}\")\n",
        "    print(f\"Columns: {list(raw_df.columns[:10])}...\")  # Show first 10 columns\n",
        "    \n",
        "    # Simple preprocessing test\n",
        "    print(\"\\nðŸ§¹ Testing basic data cleaning...\")\n",
        "    \n",
        "    # Check for missing values\n",
        "    missing_counts = raw_df.isnull().sum()\n",
        "    print(f\"Missing values found in {(missing_counts > 0).sum()} columns\")\n",
        "    \n",
        "    # Check data types\n",
        "    print(f\"Data types: {raw_df.dtypes.value_counts().to_dict()}\")\n",
        "    \n",
        "    # Basic cleaning\n",
        "    cleaned_df = raw_df.copy()\n",
        "    \n",
        "    # Remove completely empty columns\n",
        "    cleaned_df = cleaned_df.dropna(axis=1, how='all')\n",
        "    \n",
        "    # Fill missing values with median for numeric, mode for categorical\n",
        "    numeric_cols = cleaned_df.select_dtypes(include=[np.number]).columns\n",
        "    categorical_cols = cleaned_df.select_dtypes(include=['object']).columns\n",
        "    \n",
        "    print(f\"\\nFound {len(numeric_cols)} numeric and {len(categorical_cols)} categorical columns\")\n",
        "    \n",
        "    # Simple imputation\n",
        "    for col in numeric_cols:\n",
        "        if cleaned_df[col].isnull().sum() > 0:\n",
        "            cleaned_df[col] = cleaned_df[col].fillna(cleaned_df[col].median())\n",
        "    \n",
        "    for col in categorical_cols:\n",
        "        if cleaned_df[col].isnull().sum() > 0:\n",
        "            cleaned_df[col] = cleaned_df[col].fillna('Unknown')\n",
        "    \n",
        "    print(f\"âœ… Basic cleaning completed!\")\n",
        "    print(f\"Shape after cleaning: {cleaned_df.shape}\")\n",
        "    print(f\"Missing values remaining: {cleaned_df.isnull().sum().sum()}\")\n",
        "    \n",
        "else:\n",
        "    print(\"âš ï¸ No sample data available. Please run the previous cells first.\")\n",
        "    print(\"Make sure to execute Cell 3 which loads the sample data.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Data Cleaning Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_player_data(df):\n",
        "    \"\"\"\n",
        "    Comprehensive data cleaning for player datasets\n",
        "    \n",
        "    Parameters:\n",
        "    df: pandas DataFrame - raw player data\n",
        "    \n",
        "    Returns:\n",
        "    df_clean: pandas DataFrame - cleaned data\n",
        "    \"\"\"\n",
        "    df_clean = df.copy()\n",
        "    \n",
        "    print(f\"Original data shape: {df_clean.shape}\")\n",
        "    \n",
        "    # 1. Remove completely empty rows and columns\n",
        "    df_clean = df_clean.dropna(how='all')  # Remove rows with all NaN\n",
        "    df_clean = df_clean.dropna(axis=1, how='all')  # Remove columns with all NaN\n",
        "    \n",
        "    # 2. Handle duplicate records\n",
        "    initial_rows = len(df_clean)\n",
        "    df_clean = df_clean.drop_duplicates()\n",
        "    duplicates_removed = initial_rows - len(df_clean)\n",
        "    if duplicates_removed > 0:\n",
        "        print(f\"Removed {duplicates_removed} duplicate rows\")\n",
        "    \n",
        "    # 3. Clean column names (remove special characters, spaces)\n",
        "    df_clean.columns = df_clean.columns.str.strip()  # Remove whitespace\n",
        "    df_clean.columns = df_clean.columns.str.replace(' ', '_')  # Replace spaces with underscores\n",
        "    df_clean.columns = df_clean.columns.str.replace('[^A-Za-z0-9_]', '', regex=True)  # Remove special chars\n",
        "    \n",
        "    # 4. Handle missing values by data type\n",
        "    numeric_columns = df_clean.select_dtypes(include=[np.number]).columns\n",
        "    categorical_columns = df_clean.select_dtypes(include=['object']).columns\n",
        "    \n",
        "    print(f\"Numeric columns: {len(numeric_columns)}\")\n",
        "    print(f\"Categorical columns: {len(categorical_columns)}\")\n",
        "    \n",
        "    # 5. Clean numeric columns\n",
        "    for col in numeric_columns:\n",
        "        # Replace infinite values with NaN\n",
        "        df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
        "        \n",
        "        # Handle outliers (values beyond 3 standard deviations)\n",
        "        if df_clean[col].std() > 0:  # Only if there's variation\n",
        "            mean_val = df_clean[col].mean()\n",
        "            std_val = df_clean[col].std()\n",
        "            outlier_threshold = 3 * std_val\n",
        "            \n",
        "            # Cap outliers instead of removing them\n",
        "            df_clean[col] = np.where(\n",
        "                df_clean[col] > mean_val + outlier_threshold,\n",
        "                mean_val + outlier_threshold,\n",
        "                df_clean[col]\n",
        "            )\n",
        "            df_clean[col] = np.where(\n",
        "                df_clean[col] < mean_val - outlier_threshold,\n",
        "                mean_val - outlier_threshold,\n",
        "                df_clean[col]\n",
        "            )\n",
        "    \n",
        "    # 6. Clean categorical columns\n",
        "    for col in categorical_columns:\n",
        "        # Strip whitespace and standardize case\n",
        "        df_clean[col] = df_clean[col].astype(str).str.strip().str.title()\n",
        "        \n",
        "        # Replace common variations\n",
        "        df_clean[col] = df_clean[col].replace({\n",
        "            'Nan': np.nan,\n",
        "            'None': np.nan,\n",
        "            'N/A': np.nan,\n",
        "            '': np.nan\n",
        "        })\n",
        "    \n",
        "    print(f\"Cleaned data shape: {df_clean.shape}\")\n",
        "    \n",
        "    return df_clean\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def handle_missing_values(df, strategy='smart'):\n",
        "    \"\"\"\n",
        "    Handle missing values using different strategies\n",
        "    \n",
        "    Parameters:\n",
        "    df: pandas DataFrame\n",
        "    strategy: str - 'smart', 'median', 'mean', 'mode', 'drop'\n",
        "    \n",
        "    Returns:\n",
        "    df_imputed: pandas DataFrame with missing values handled\n",
        "    \"\"\"\n",
        "    df_imputed = df.copy()\n",
        "    \n",
        "    # Get missing value statistics\n",
        "    missing_stats = df_imputed.isnull().sum()\n",
        "    missing_percent = (missing_stats / len(df_imputed)) * 100\n",
        "    \n",
        "    print(\"Missing Value Analysis:\")\n",
        "    print(\"=\" * 40)\n",
        "    for col in missing_stats[missing_stats > 0].index:\n",
        "        print(f\"{col}: {missing_stats[col]} ({missing_percent[col]:.1f}%)\")\n",
        "    \n",
        "    if strategy == 'smart':\n",
        "        # Smart imputation based on data type and missing percentage\n",
        "        for col in df_imputed.columns:\n",
        "            missing_pct = missing_percent[col]\n",
        "            \n",
        "            if missing_pct > 70:\n",
        "                # Drop columns with >70% missing data\n",
        "                print(f\"Dropping {col} - too many missing values ({missing_pct:.1f}%)\")\n",
        "                df_imputed = df_imputed.drop(columns=[col])\n",
        "                \n",
        "            elif missing_pct > 0:\n",
        "                if df_imputed[col].dtype in ['int64', 'float64']:\n",
        "                    # For numeric: use median (robust to outliers)\n",
        "                    df_imputed[col] = df_imputed[col].fillna(df_imputed[col].median())\n",
        "                    print(f\"Filled {col} with median value\")\n",
        "                    \n",
        "                else:\n",
        "                    # For categorical: use mode or 'Unknown'\n",
        "                    if not df_imputed[col].mode().empty:\n",
        "                        df_imputed[col] = df_imputed[col].fillna(df_imputed[col].mode()[0])\n",
        "                        print(f\"Filled {col} with mode value\")\n",
        "                    else:\n",
        "                        df_imputed[col] = df_imputed[col].fillna('Unknown')\n",
        "                        print(f\"Filled {col} with 'Unknown'\")\n",
        "    \n",
        "    elif strategy == 'median':\n",
        "        # Use median for numeric, mode for categorical\n",
        "        numeric_cols = df_imputed.select_dtypes(include=[np.number]).columns\n",
        "        categorical_cols = df_imputed.select_dtypes(include=['object']).columns\n",
        "        \n",
        "        for col in numeric_cols:\n",
        "            df_imputed[col] = df_imputed[col].fillna(df_imputed[col].median())\n",
        "        \n",
        "        for col in categorical_cols:\n",
        "            df_imputed[col] = df_imputed[col].fillna(df_imputed[col].mode()[0] if not df_imputed[col].mode().empty else 'Unknown')\n",
        "    \n",
        "    elif strategy == 'drop':\n",
        "        # Drop rows with any missing values\n",
        "        df_imputed = df_imputed.dropna()\n",
        "        print(f\"Dropped rows with missing values. New shape: {df_imputed.shape}\")\n",
        "    \n",
        "    print(f\"Final shape after handling missing values: {df_imputed.shape}\")\n",
        "    return df_imputed\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Data Normalization and Scaling Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_data(df, method='standard', exclude_columns=None):\n",
        "    \"\"\"\n",
        "    Normalize/scale numerical data\n",
        "    \n",
        "    Parameters:\n",
        "    df: pandas DataFrame\n",
        "    method: str - 'standard', 'minmax', 'robust', 'quantile'\n",
        "    exclude_columns: list - columns to exclude from scaling\n",
        "    \n",
        "    Returns:\n",
        "    df_scaled: pandas DataFrame with scaled features\n",
        "    scaler: fitted scaler object\n",
        "    \"\"\"\n",
        "    df_scaled = df.copy()\n",
        "    \n",
        "    # Identify numeric columns\n",
        "    numeric_columns = df_scaled.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    \n",
        "    # Remove excluded columns\n",
        "    if exclude_columns:\n",
        "        numeric_columns = [col for col in numeric_columns if col not in exclude_columns]\n",
        "    \n",
        "    print(f\"Scaling {len(numeric_columns)} numeric columns using {method} method\")\n",
        "    \n",
        "    # Select appropriate scaler\n",
        "    if method == 'standard':\n",
        "        scaler = StandardScaler()\n",
        "        print(\"Using StandardScaler (mean=0, std=1)\")\n",
        "        \n",
        "    elif method == 'minmax':\n",
        "        scaler = MinMaxScaler()\n",
        "        print(\"Using MinMaxScaler (range 0-1)\")\n",
        "        \n",
        "    elif method == 'robust':\n",
        "        from sklearn.preprocessing import RobustScaler\n",
        "        scaler = RobustScaler()\n",
        "        print(\"Using RobustScaler (median-based, handles outliers)\")\n",
        "        \n",
        "    elif method == 'quantile':\n",
        "        from sklearn.preprocessing import QuantileTransformer\n",
        "        scaler = QuantileTransformer(output_distribution='normal')\n",
        "        print(\"Using QuantileTransformer (uniform to normal distribution)\")\n",
        "    \n",
        "    # Fit and transform the data\n",
        "    if numeric_columns:\n",
        "        df_scaled[numeric_columns] = scaler.fit_transform(df_scaled[numeric_columns])\n",
        "        \n",
        "        # Display scaling statistics\n",
        "        print(\"\\\\nScaling Statistics:\")\n",
        "        print(\"=\" * 50)\n",
        "        for col in numeric_columns[:5]:  # Show first 5 columns\n",
        "            original_mean = df[col].mean()\n",
        "            original_std = df[col].std()\n",
        "            scaled_mean = df_scaled[col].mean()\n",
        "            scaled_std = df_scaled[col].std()\n",
        "            \n",
        "            print(f\"{col}:\")\n",
        "            print(f\"  Original: mean={original_mean:.3f}, std={original_std:.3f}\")\n",
        "            print(f\"  Scaled:   mean={scaled_mean:.3f}, std={scaled_std:.3f}\")\n",
        "        \n",
        "        if len(numeric_columns) > 5:\n",
        "            print(f\"... and {len(numeric_columns)-5} more columns\")\n",
        "    \n",
        "    return df_scaled, scaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_categorical_data(df, encoding_method='label', high_cardinality_threshold=10):\n",
        "    \"\"\"\n",
        "    Encode categorical variables\n",
        "    \n",
        "    Parameters:\n",
        "    df: pandas DataFrame\n",
        "    encoding_method: str - 'label', 'onehot', 'target', 'binary'\n",
        "    high_cardinality_threshold: int - threshold for high cardinality features\n",
        "    \n",
        "    Returns:\n",
        "    df_encoded: pandas DataFrame with encoded features\n",
        "    encoders: dict - fitted encoder objects\n",
        "    \"\"\"\n",
        "    df_encoded = df.copy()\n",
        "    encoders = {}\n",
        "    \n",
        "    # Identify categorical columns\n",
        "    categorical_columns = df_encoded.select_dtypes(include=['object']).columns.tolist()\n",
        "    \n",
        "    print(f\"Encoding {len(categorical_columns)} categorical columns\")\n",
        "    print(f\"Method: {encoding_method}\")\n",
        "    \n",
        "    for col in categorical_columns:\n",
        "        unique_values = df_encoded[col].nunique()\n",
        "        print(f\"\\\\n{col}: {unique_values} unique values\")\n",
        "        \n",
        "        if unique_values <= 1:\n",
        "            # Drop columns with no variation\n",
        "            print(f\"  Dropping {col} - no variation\")\n",
        "            df_encoded = df_encoded.drop(columns=[col])\n",
        "            continue\n",
        "        \n",
        "        if encoding_method == 'label':\n",
        "            # Label encoding\n",
        "            le = LabelEncoder()\n",
        "            df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
        "            encoders[col] = le\n",
        "            print(f\"  Applied Label Encoding\")\n",
        "            \n",
        "        elif encoding_method == 'onehot':\n",
        "            if unique_values <= high_cardinality_threshold:\n",
        "                # One-hot encoding for low cardinality\n",
        "                dummies = pd.get_dummies(df_encoded[col], prefix=col)\n",
        "                df_encoded = pd.concat([df_encoded, dummies], axis=1)\n",
        "                df_encoded = df_encoded.drop(columns=[col])\n",
        "                encoders[col] = list(dummies.columns)\n",
        "                print(f\"  Applied One-Hot Encoding ({len(dummies.columns)} new columns)\")\n",
        "            else:\n",
        "                # Label encoding for high cardinality\n",
        "                le = LabelEncoder()\n",
        "                df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
        "                encoders[col] = le\n",
        "                print(f\"  Applied Label Encoding (high cardinality)\")\n",
        "        \n",
        "        elif encoding_method == 'binary':\n",
        "            # Binary encoding (good for medium cardinality)\n",
        "            try:\n",
        "                import category_encoders as ce\n",
        "                be = ce.BinaryEncoder(cols=[col])\n",
        "                encoded_cols = be.fit_transform(df_encoded[col])\n",
        "                df_encoded = pd.concat([df_encoded, encoded_cols], axis=1)\n",
        "                df_encoded = df_encoded.drop(columns=[col])\n",
        "                encoders[col] = be\n",
        "                print(f\"  Applied Binary Encoding ({len(encoded_cols.columns)} new columns)\")\n",
        "            except ImportError:\n",
        "                print(f\"  Binary encoding not available, using Label Encoding\")\n",
        "                le = LabelEncoder()\n",
        "                df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
        "                encoders[col] = le\n",
        "    \n",
        "    print(f\"\\\\nFinal shape after encoding: {df_encoded.shape}\")\n",
        "    return df_encoded, encoders\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Complete Preprocessing Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def complete_preprocessing_pipeline(df, \n",
        "                                   clean_data=True,\n",
        "                                   handle_missing='smart',\n",
        "                                   normalize_method='standard',\n",
        "                                   encoding_method='label',\n",
        "                                   exclude_from_scaling=None):\n",
        "    \"\"\"\n",
        "    Complete preprocessing pipeline that combines all steps\n",
        "    \n",
        "    Parameters:\n",
        "    df: pandas DataFrame - raw data\n",
        "    clean_data: bool - whether to clean the data\n",
        "    handle_missing: str - missing value strategy\n",
        "    normalize_method: str - scaling method\n",
        "    encoding_method: str - categorical encoding method\n",
        "    exclude_from_scaling: list - columns to exclude from scaling\n",
        "    \n",
        "    Returns:\n",
        "    processed_df: pandas DataFrame - fully processed data\n",
        "    preprocessing_info: dict - information about preprocessing steps\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"ðŸš€ STARTING COMPLETE PREPROCESSING PIPELINE\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    processed_df = df.copy()\n",
        "    preprocessing_info = {\n",
        "        'original_shape': df.shape,\n",
        "        'steps_applied': [],\n",
        "        'encoders': {},\n",
        "        'scaler': None\n",
        "    }\n",
        "    \n",
        "    # Step 1: Data Cleaning\n",
        "    if clean_data:\n",
        "        print(\"\\\\nðŸ“‹ STEP 1: DATA CLEANING\")\n",
        "        print(\"-\" * 30)\n",
        "        processed_df = clean_player_data(processed_df)\n",
        "        preprocessing_info['steps_applied'].append('data_cleaning')\n",
        "    \n",
        "    # Step 2: Handle Missing Values\n",
        "    if handle_missing:\n",
        "        print(\"\\\\nðŸ”§ STEP 2: HANDLING MISSING VALUES\")\n",
        "        print(\"-\" * 30)\n",
        "        processed_df = handle_missing_values(processed_df, strategy=handle_missing)\n",
        "        preprocessing_info['steps_applied'].append(f'missing_values_{handle_missing}')\n",
        "    \n",
        "    # Step 3: Encode Categorical Variables\n",
        "    if encoding_method:\n",
        "        print(\"\\\\nðŸ·ï¸ STEP 3: ENCODING CATEGORICAL VARIABLES\")\n",
        "        print(\"-\" * 30)\n",
        "        processed_df, encoders = encode_categorical_data(processed_df, encoding_method=encoding_method)\n",
        "        preprocessing_info['encoders'] = encoders\n",
        "        preprocessing_info['steps_applied'].append(f'encoding_{encoding_method}')\n",
        "    \n",
        "    # Step 4: Normalize/Scale Data\n",
        "    if normalize_method:\n",
        "        print(\"\\\\nðŸ“Š STEP 4: NORMALIZING DATA\")\n",
        "        print(\"-\" * 30)\n",
        "        processed_df, scaler = normalize_data(processed_df, \n",
        "                                            method=normalize_method,\n",
        "                                            exclude_columns=exclude_from_scaling)\n",
        "        preprocessing_info['scaler'] = scaler\n",
        "        preprocessing_info['steps_applied'].append(f'scaling_{normalize_method}')\n",
        "    \n",
        "    # Final summary\n",
        "    preprocessing_info['final_shape'] = processed_df.shape\n",
        "    \n",
        "    print(\"\\\\nâœ… PREPROCESSING PIPELINE COMPLETED\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Original shape: {preprocessing_info['original_shape']}\")\n",
        "    print(f\"Final shape: {preprocessing_info['final_shape']}\")\n",
        "    print(f\"Steps applied: {', '.join(preprocessing_info['steps_applied'])}\")\n",
        "    \n",
        "    return processed_df, preprocessing_info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== EXAMPLE: PREPROCESSING PIPELINE USAGE ===\n",
            "Loading players.csv...\n",
            "  âœ“ Loaded 5942 records from 157 teams\n",
            "\\nProcessing players dataset...\n",
            "Original shape: (5942, 38)\n",
            "ðŸš€ STARTING COMPLETE PREPROCESSING PIPELINE\n",
            "============================================================\n",
            "\\nðŸ“‹ STEP 1: DATA CLEANING\n",
            "------------------------------\n",
            "Original data shape: (5942, 38)\n",
            "Numeric columns: 30\n",
            "Categorical columns: 8\n",
            "Cleaned data shape: (5942, 38)\n",
            "\\nðŸ”§ STEP 2: HANDLING MISSING VALUES\n",
            "------------------------------\n",
            "Missing Value Analysis:\n",
            "========================================\n",
            "Playing_Time_MP: 4916 (82.7%)\n",
            "MP: 1026 (17.3%)\n",
            "Dropping Playing_Time_MP - too many missing values (82.7%)\n",
            "Filled MP with median value\n",
            "Final shape after handling missing values: (5942, 37)\n",
            "\\nðŸ·ï¸ STEP 3: ENCODING CATEGORICAL VARIABLES\n",
            "------------------------------\n",
            "Encoding 8 categorical columns\n",
            "Method: label\n",
            "\\nPlayer: 5192 unique values\n",
            "  Applied Label Encoding\n",
            "\\nNation: 120 unique values\n",
            "  Applied Label Encoding\n",
            "\\nPos: 11 unique values\n",
            "  Applied Label Encoding\n",
            "\\nAge: 1310 unique values\n",
            "  Applied Label Encoding\n",
            "\\nMatches: 2 unique values\n",
            "  Applied Label Encoding\n",
            "\\nLeague: 8 unique values\n",
            "  Applied Label Encoding\n",
            "\\nTeam: 157 unique values\n",
            "  Applied Label Encoding\n",
            "\\nData_Source: 157 unique values\n",
            "  Applied Label Encoding\n",
            "\\nFinal shape after encoding: (5942, 37)\n",
            "\\nðŸ“Š STEP 4: NORMALIZING DATA\n",
            "------------------------------\n",
            "Scaling 35 numeric columns using standard method\n",
            "Using StandardScaler (mean=0, std=1)\n",
            "\\nScaling Statistics:\n",
            "==================================================\n",
            "Player:\n",
            "  Original: mean=2674.792, std=1504.407\n",
            "  Scaled:   mean=-0.000, std=1.000\n",
            "Nation:\n",
            "  Original: mean=45.062, std=29.741\n",
            "  Scaled:   mean=-0.000, std=1.000\n",
            "Pos:\n",
            "  Original: mean=4.742, std=3.229\n",
            "  Scaled:   mean=0.000, std=1.000\n",
            "Age:\n",
            "  Original: mean=571.342, std=373.458\n",
            "  Scaled:   mean=-0.000, std=1.000\n",
            "Playing_Time_Starts:\n",
            "  Original: mean=23.467, std=58.577\n",
            "  Scaled:   mean=0.000, std=1.000\n",
            "... and 30 more columns\n",
            "\\nâœ… PREPROCESSING PIPELINE COMPLETED\n",
            "============================================================\n",
            "Original shape: (5942, 38)\n",
            "Final shape: (5942, 37)\n",
            "Steps applied: data_cleaning, missing_values_smart, encoding_label, scaling_standard\n",
            "\\nðŸ“ˆ PREPROCESSING SUMMARY:\n",
            "Shape change: (5942, 38) â†’ (5942, 37)\n",
            "Columns reduced/added: 1\n",
            "\\nðŸ“‹ SAMPLE OF PROCESSED DATA:\n",
            "     Player    Nation       Pos       Age  Playing_Time_Starts  \\\n",
            "0  1.522461 -0.102969 -1.158973 -0.582021             0.248127   \n",
            "1 -1.044880 -0.203850  1.008967 -0.057152             0.231054   \n",
            "2  0.383046  1.410244  1.008967 -0.057152             0.196908   \n",
            "\n",
            "   Playing_Time_Min  Playing_Time_90s  Performance_Gls  Performance_Ast  \\\n",
            "0          2.206825          2.206736        -0.120013        -0.197348   \n",
            "1          2.026485          2.023626         0.514998         1.078076   \n",
            "2          1.902559          1.898778         0.642000         1.442483   \n",
            "\n",
            "   Performance_GA  ...  Per_90_Minutes_xG  Per_90_Minutes_xAG  \\\n",
            "0       -0.153405  ...          -0.413475           -0.501524   \n",
            "1        0.757554  ...          -0.244346            0.179270   \n",
            "2        0.985294  ...           0.195389            0.811437   \n",
            "\n",
            "   Per_90_Minutes_xGxAG  Per_90_Minutes_npxG  Per_90_Minutes_npxGxAG  \\\n",
            "0             -0.481335            -0.406231               -0.479461   \n",
            "1             -0.071995            -0.224114               -0.051437   \n",
            "2              0.488153             0.067274                0.421642   \n",
            "\n",
            "    Matches  League  Team  Data_Source        MP  \n",
            "0  0.236601       2     7     -0.93763 -0.227671  \n",
            "1  0.236601       2     7     -0.93763 -0.227671  \n",
            "2  0.236601       2     7     -0.93763 -0.227671  \n",
            "\n",
            "[3 rows x 37 columns]\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Example usage of the complete preprocessing pipeline\n",
        "print(\"=== EXAMPLE: PREPROCESSING PIPELINE USAGE ===\")\n",
        "\n",
        "# Load sample data\n",
        "sample_data = load_all_data(\"players.csv\")\n",
        "\n",
        "if sample_data:\n",
        "    # Get the first dataset\n",
        "    data_key = list(sample_data.keys())[0]\n",
        "    raw_df = sample_data[data_key]\n",
        "    \n",
        "    print(f\"\\\\nProcessing {data_key} dataset...\")\n",
        "    print(f\"Original shape: {raw_df.shape}\")\n",
        "    \n",
        "    # Apply complete preprocessing pipeline\n",
        "    processed_df, info = complete_preprocessing_pipeline(\n",
        "        raw_df,\n",
        "        clean_data=True,\n",
        "        handle_missing='smart',\n",
        "        normalize_method='standard',\n",
        "        encoding_method='label',\n",
        "        exclude_from_scaling=['League', 'Team']  # Don't scale these if they exist\n",
        "    )\n",
        "    \n",
        "    print(f\"\\\\nðŸ“ˆ PREPROCESSING SUMMARY:\")\n",
        "    print(f\"Shape change: {info['original_shape']} â†’ {info['final_shape']}\")\n",
        "    print(f\"Columns reduced/added: {info['original_shape'][1] - info['final_shape'][1]}\")\n",
        "    \n",
        "    # Show sample of processed data\n",
        "    print(f\"\\\\nðŸ“‹ SAMPLE OF PROCESSED DATA:\")\n",
        "    print(processed_df.head(3))\n",
        "    \n",
        "else:\n",
        "    print(\"No sample data loaded. Please check your data directory.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
